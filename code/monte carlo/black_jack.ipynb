{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict,deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Black Jack Environment\n",
    "> Actions => 1: continue 0: stop\n",
    "\n",
    "> States => (a,b,c) a:hand, b:table open card c: usable ace (True|False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 7, True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Episode\n",
    "def random(env):\n",
    "    '''\n",
    "    Chooses actions completely randomly independent from the current state\n",
    "    args:\n",
    "        env: Black Jack Environment\n",
    "    returns:\n",
    "        episode list: list of state,action,reward pairs\n",
    "        rewards list: list of rewards for the episode created. Last value is the reward 1,-1 or 0 previous rewards are 0\n",
    "    '''\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        action = np.random.choice((1,0)) # randomly chooce 1 or 0\n",
    "        current_state,reward,done,info = env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = current_state\n",
    "        rewards.append(reward)\n",
    "        if done: # episode ends with a reward\n",
    "            break\n",
    "    return episode,rewards\n",
    "# \n",
    "def stoch(env):\n",
    "    '''\n",
    "    Chooses actions by a hard coded probability. If current sum is > 17 9 times out of 10 will choose action 0\n",
    "    '''\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        if state[0]>17: \n",
    "            p = [0.1,0.9]\n",
    "        else:\n",
    "            p = [0.5,0.5]\n",
    "        action = np.random.choice((0,1),p=p) # choose action with a probability\n",
    "        current_state,reward,done,info = env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = current_state\n",
    "        rewards.append(reward)\n",
    "        if done: # episode ends with a reward\n",
    "            break\n",
    "    return episode,rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode_func(env,episode_gen_function,iterations,decay_factor=0.9):\n",
    "    '''\n",
    "    Evaluates the episode generation function and policy \n",
    "    args:\n",
    "        env \n",
    "        episode_gen_function\n",
    "        iterations: number of episodes to be created. \n",
    "        decay factor float: Rewards are given at the end of the episode. So if the state occured at the very beginning\n",
    "        then it should have a decayed reward. \n",
    "    returns:\n",
    "        Q defaultdict : cumilative sum of action state pair values\n",
    "        N defaultdict : number of times a given state action pair occured\n",
    "        V defaultdict : average value of a given state action pair\n",
    "    '''\n",
    "    total_wins = 0 \n",
    "    Q=defaultdict(lambda : np.zeros(2)) # action state value colection\n",
    "    N = defaultdict(lambda : np.zeros(2)) # store action state pair counts\n",
    "    V = defaultdict(lambda : np.zeros(2))  # store average action state values\n",
    "    for iteration in range(iterations):\n",
    "        episode,rewards = episode_gen_function(env) # actual reward is given in the end of the episode\n",
    "        if rewards[-1] == 1:\n",
    "            total_wins +=1\n",
    "        decay =[decay_factor**k for k in  range(len(episode))]\n",
    "        decay = deque(decay) # deque is faster\n",
    "        for inst in episode:\n",
    "            state = inst[0]\n",
    "            action = inst[1]\n",
    "            Q[state][action] += rewards[-1]*decay.pop() # this is a right removal so that we read the decay in a reverse order\n",
    "            N[state][action] +=1\n",
    "    # calculate average state action value\n",
    "    for state,value in Q.items():\n",
    "    #print(state,value)\n",
    "        for action in range(2):\n",
    "            if N[state][action] !=0:\n",
    "                V[state][action] = value[action]/N[state][action]\n",
    "    return Q,N,V,total_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q,N,V,total_wins = evaluate_episode_func(env,random,iterations=500000,decay_factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total wins are 140419 \n",
      "percentage wins 0.280838 \n"
     ]
    }
   ],
   "source": [
    "print('total wins are {} \\npercentage wins {} '.format(total_wins,total_wins/500000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q,N,V,total_wins = evaluate_episode_func(env,stoch,iterations=500000,decay_factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total wins are 81965 \n",
      "percentage wins 0.16393 \n"
     ]
    }
   ],
   "source": [
    "print('total wins are {} \\npercentage wins {} '.format(total_wins,total_wins/500000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Policy Using Monte Carlo Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state,V,epsilon,nA):\n",
    "    '''\n",
    "    Chooses an action using history up to the current point\n",
    "    args:\n",
    "        state tuple\n",
    "        V defaultdict : Action State pair average value\n",
    "        epsilon float : between 0 and 1 metric to set how much to eplore and how much to exploit\n",
    "        nA int : number of actions available\n",
    "    returns:\n",
    "        action int\n",
    "    '''\n",
    "    probs = epsilon*np.ones(nA)/nA # initiate probs its sum is equal to epsilon\n",
    "    if sum(V[state] ==0) !=nA: # check if we have zero value for all the actions\n",
    "        best_action = np.argmax(V[state]) # get the best action based on the history so far\n",
    "        prob_best_action = 1-epsilon # probability of best action is equal to 1 - epsilon\n",
    "        probs[best_action] = prob_best_action+epsilon/nA # sum of probs is equal to 1 \n",
    "        action = np.random.choice(np.arange(nA),p=probs)\n",
    "    else:\n",
    "        action = np.random.choice(np.arange(nA))\n",
    "    return action\n",
    "\n",
    "def create_episode(env,V,epsilon,nA):\n",
    "    '''\n",
    "    create an episode by using actions chosen by get action function\n",
    "    args:\n",
    "        env\n",
    "        V\n",
    "        epsilon\n",
    "        nA\n",
    "    returns:\n",
    "        episode\n",
    "        rewards\n",
    "    '''\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        action = get_action(state,V,epsilon,nA) # use get action\n",
    "        current_state,reward,done,info = env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = current_state\n",
    "        rewards.append(reward)\n",
    "        if done: # episode ends with a reward\n",
    "            break\n",
    "    return episode,rewards\n",
    "\n",
    "def collect_values(env,epsilon,nA,iterations,episode_gen_function,decay_factor):\n",
    "    Q = defaultdict(lambda : np.zeros(nA)) # action state value colection\n",
    "    N = defaultdict(lambda : np.zeros(nA)) # store action state pair counts\n",
    "    V = defaultdict(lambda : np.zeros(nA))  # store average action state values\n",
    "    for iteration in range(iterations):\n",
    "        episode,rewards = episode_gen_function(env,V,epsilon,nA) # actual reward is given in the end of the episode\n",
    "        decay =[decay_factor**k for k in  range(len(episode))]\n",
    "        decay = deque(decay) # deque is faster\n",
    "        for inst in episode:\n",
    "            state = inst[0]\n",
    "            action = inst[1]\n",
    "            Q[state][action] += rewards[-1]*decay.pop() # this is a right removal so that we read the decay in a reverse order\n",
    "            N[state][action] +=1\n",
    "    # calculate average state action value\n",
    "    for state,value in Q.items():\n",
    "    #print(state,value)\n",
    "        for action in range(2):\n",
    "            if N[state][action] !=0:\n",
    "                V[state][action] = value[action]/N[state][action]\n",
    "    return Q,N,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q,N,V = collect_values(env,epsilon=0.05,nA=2,iterations=500000,episode_gen_function=create_episode,decay_factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy={}\n",
    "for key,value in V.items():\n",
    "    Policy[key]=np.argmax(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_from_policy(state,Policy,nA):\n",
    "    try:\n",
    "        action = Policy[state]\n",
    "    except:\n",
    "        action = np.random.choice(np.arange(nA))\n",
    "    return action\n",
    "\n",
    "\n",
    "def create_episode(env,Policy,nA):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        action = select_action_from_policy(state,Policy,nA)\n",
    "        current_state,reward,done,info = env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = current_state\n",
    "        rewards.append(reward)\n",
    "        if done: # episode ends with a reward\n",
    "            break\n",
    "    return episode,rewards    \n",
    "\n",
    "def run_episodes(env,Policy,nA,num_iterations):\n",
    "    total_wins = 0\n",
    "    for k in range(num_iterations):\n",
    "        episode,rewards = create_episode(env,Policy,nA)\n",
    "        if rewards[-1] == 1:\n",
    "            total_wins +=1\n",
    "    return total_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([((5, 1, False), 1, 0), ((15, 1, False), 0, -1.0)], [0, -1.0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_episode(env,Policy,nA=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate created policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wins = run_episodes(env,Policy,2,500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total wins are 207211 \n",
      "percentage wins 0.414422 \n"
     ]
    }
   ],
   "source": [
    "print('total wins are {} \\npercentage wins {} '.format(total_wins,total_wins/500000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
